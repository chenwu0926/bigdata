# Flume 配置文件
# 用于收集日志文件并发送到 Kafka

# 定义 Agent 名称
agent.sources = tailSource
agent.channels = memChannel
agent.sinks = kafkaSink

# Source 配置：使用 exec 监控日志文件
agent.sources.tailSource.type = exec
agent.sources.tailSource.command = tail -F /var/log/nginx/access.log
agent.sources.tailSource.channels = memChannel
agent.sources.tailSource.restart = true
agent.sources.tailSource.restartThrottle = 10000

# 使用 Taildir Source（推荐，支持断点续传）
# agent.sources.tailSource.type = TAILDIR
# agent.sources.tailSource.positionFile = /var/log/flume/taildir_position.json
# agent.sources.tailSource.filegroups = f1 f2
# agent.sources.tailSource.filegroups.f1 = /var/log/nginx/access.log
# agent.sources.tailSource.filegroups.f2 = /opt/tomcat/logs/localhost_access_log.*.txt

# Channel 配置：内存通道
agent.channels.memChannel.type = memory
agent.channels.memChannel.capacity = 10000
agent.channels.memChannel.transactionCapacity = 1000

# Sink 配置：Kafka
agent.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.kafkaSink.kafka.bootstrap.servers = localhost:9092
agent.sinks.kafkaSink.kafka.topic = imooc-access-log
agent.sinks.kafkaSink.kafka.producer.acks = 1
agent.sinks.kafkaSink.kafka.producer.linger.ms = 1
agent.sinks.kafkaSink.kafka.producer.compression.type = snappy
agent.sinks.kafkaSink.channel = memChannel
agent.sinks.kafkaSink.flumeBatchSize = 100

# 启动命令:
# flume-ng agent --name agent --conf-file flume-kafka.conf -Dflume.root.logger=INFO,console
